// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//	http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//
// Tries to use the guidelines at https://cloud.google.com/apis/design for the gRPC API where possible.
//
// TODO: permissive deadlines for all RPC calls
package main

import (
	"context"
	"fmt"
	"io"
	"math"
	"os"
	"os/signal"
	"regexp"
	"strconv"
	"strings"
	"sync"
	"time"

	"go.opentelemetry.io/otel/attribute"
	"go.opentelemetry.io/otel/metric"
	"google.golang.org/genproto/googleapis/rpc/errdetails"
	"google.golang.org/grpc"
	"google.golang.org/grpc/codes"
	"google.golang.org/grpc/credentials/insecure"
	"google.golang.org/grpc/status"
	"google.golang.org/protobuf/proto"
	"google.golang.org/protobuf/types/known/timestamppb"

	"net/http"
	_ "net/http"       // Debug
	_ "net/http/pprof" // Debug

	pb "open-match.dev/pkg/pb/v2"

	"github.com/davecgh/go-spew/spew"
	"github.com/pkg/errors"
	"github.com/sirupsen/logrus"
	"github.com/spf13/viper"
	"open-match.dev/core/internal/config"
	"open-match.dev/core/internal/filter"
	"open-match.dev/core/internal/statestore/cache"
	store "open-match.dev/core/internal/statestore/datatypes"
	memoryReplicator "open-match.dev/core/internal/statestore/memory"
	redisReplicator "open-match.dev/core/internal/statestore/redis"
)

// Required by protobuf compiler's golang gRPC auto-generated code.
type grpcServer struct {
	pb.UnimplementedOpenMatchServiceServer
}

var (
	logger = logrus.WithFields(logrus.Fields{
		"app":       "open_match",
		"component": "core",
	})
	cfg *viper.Viper = nil

	// One global instance for the local ticket cache. Everything reads and
	// writes to this one instance which contains concurrent-safe data
	// structures where necessary.
	tc cache.ReplicatedTicketCache
)

func main() {

	// DEBUG serve the http/pprof module debug endpoint
	go func() {
		logger.Print(http.ListenAndServe("localhost:2224", nil))
	}()

	// Read configuration env vars, and configure logging
	cfg = config.Read()

	// Make a parent context that gets cancelled on SIGINT
	ctx, cancel := signal.NotifyContext(context.Background(), os.Interrupt)
	defer cancel()

	// Configure metrics
	meter := initializeOtel()
	registerMetrics(meter)
	cache.RegisterMetrics(meter)

	// Set up the replicated ticket cache.
	// fields using sync.Map come ready to use and don't need initialization
	tc.Cfg = cfg
	tc.UpRequests = make(chan *cache.UpdateRequest)
	switch cfg.GetString("OM_STATE_STORAGE_TYPE") {
	case "redis":
		// Default: use redis
		tc.Replicator = redisReplicator.New(cfg)
	case "memory":
		// use local memory
		// NOT RECOMMENDED FOR PRODUCTION
		//
		// Store statage in local memory only. Every copy of om-core spun up
		// using this configuration is an island which does not send or receive
		// any updates to/from any other instances.
		// Useful for debugging, local development, etc.
		logger.Warnf("OM_STATE_STORAGE_TYPE configuration variable set to 'memory'. NOT RECOMMENDED FOR PRODUCTION")
		tc.Replicator = memoryReplicator.New(cfg)
	}

	// These goroutines send and receive cache updates from state storage
	go tc.OutgoingReplicationQueue(ctx)
	go tc.IncomingReplicationQueue(ctx)

	// Start the gRPC server
	start(cfg)

	// TODO remove for RC
	// Dump the final state of the cache to the log for debugging.
	if cfg.GetBool("OM_VERBOSE") {
		spew.Dump(syncMapDump(&tc.Tickets))
		spew.Dump(syncMapDump(&tc.InactiveSet))
		spew.Dump(syncMapDump(&tc.Assignments))
	}
	logger.Info("Application stopped successfully.")
	logger.Infof("Final state of local cache: %v tickets, %v active, %v inactive, %v assignments", len(syncMapDump(&tc.Tickets)), len(setDifference(&tc.Tickets, &tc.InactiveSet)), len(syncMapDump(&tc.InactiveSet)), len(syncMapDump(&tc.Assignments)))

}

// CreateTicket generates an event to update the ticket state storage, adding a
// new ticket. The ticket's id will be generated by the state storage and
// returned asynchronously.  This request hangs until it can return the ticket
// id. When a ticket is created, it starts off as inactive and must be
// activated with the ActivateTickets call. This ensures that no ticket that
// was not successfully replicated and returned to the om-core client is ever
// put in the pool.
func (s *grpcServer) CreateTicket(parentCtx context.Context, req *pb.CreateTicketRequest) (*pb.CreateTicketResponse, error) {
	logger := logger.WithFields(logrus.Fields{
		"stage": "handling",
		"rpc":   "CreateTicket",
	})

	// Input validation
	if req.GetTicket() == nil {
		return nil, status.Errorf(codes.InvalidArgument, "ticket is required")
	}
	if req.GetTicket().GetId() != "" {
		logger.Warnf("CreateTicket request included a ticketid (%v). Open Match assigns Ticket IDs, so this value will be overwritten. See documentation for more details.", req.GetTicket().GetId())
	}

	// Get timestamps from ticket in the request
	crTime := req.GetTicket().GetAttributes().GetCreationTime()
	exTime := req.GetTicket().GetExpirationTime()

	// Set creation timestamp if request didn't provide one
	if !crTime.IsValid() {
		logger.Info("CreationTime provided is invalid or nil; replacing with current time")
		crTime = timestamppb.Now()
	}

	// The default expiration time is based on the provided creation time
	// (which defaults to now) + the configured ticket TTL
	defaultExTime := crTime.AsTime().Add(time.Second * time.Duration(cfg.GetInt("OM_CACHE_TICKET_TTL_SECS")))

	// Set expiration timestamp if request didn't provide a valid one.
	// Matchmakers can request a shorter expiration time than the default, but
	// not a longer one.
	if !exTime.IsValid() ||
		exTime.AsTime().After(defaultExTime) {
		logger.Debugf("ExpirationTime provided is invalid or nil; replacing with current time + OM_CACHE_TICKET_TTL_SECS (%v)", cfg.GetInt("OM_CACHE_TICKET_TTL_SECS"))
		exTime = timestamppb.New(defaultExTime)
	}

	// Update ticket with new creation/expiration time
	ticketPb, err := proto.Marshal(&pb.Ticket{
		ExpirationTime: exTime,
		Extensions:     req.GetTicket().GetExtensions(),
		Attributes: &pb.Ticket_FilterableData{
			Tags:         req.GetTicket().GetAttributes().GetTags(),
			StringArgs:   req.GetTicket().GetAttributes().GetStringArgs(),
			DoubleArgs:   req.GetTicket().GetAttributes().GetDoubleArgs(),
			CreationTime: crTime,
		},
	})

	// Marshal ticket into storage format
	if ticketPb == nil || err != nil {
		err = errors.Wrap(err, "failed to marshal the ticket protobuf")
		logger.Errorf(" CreateTicket: %v", err)
		return nil, err
	}

	// Make a results return channel
	rChan := make(chan *store.StateResponse)

	// Queue our ticket creation cache update request to be replicated to all
	// om-core instances.
	tc.UpRequests <- &cache.UpdateRequest{
		// This command (writing a ticket to the cache)
		// is replicated to all other om-core instances using the batch
		// writing async goroutine tc.OutgoingReplicationQueue() and its
		// effect is applied to the local ticket cache in the update
		// processing async goroutine tc.IncomingReplicationQueue().
		ResultsChan: rChan,
		Ctx:         parentCtx,
		Update: store.StateUpdate{
			Cmd:   store.Ticket,
			Value: string(ticketPb[:]),
		},
	}

	// Get the results
	results := <-rChan

	return &pb.CreateTicketResponse{TicketId: results.Result}, results.Err
}

// updateTicketsActiveState accepts a list of ticketids to (de-)activate, and
// generates cache updates for each.
//
// NOTE This function does no input validation, that responsibility falls on the
// calling function.
func updateTicketsActiveState(parentCtx context.Context, ticketIds []string, command int) map[string]error {
	logger := logger.WithFields(logrus.Fields{
		"stage": "handling",
		"rpc":   "updateTicketsActiveState",
	})

	errs := map[string]error{}

	rChan := make(chan *store.StateResponse, len(ticketIds))
	defer close(rChan)

	// Make a human-readable version of the requested state transition, for
	// logging
	requestedStateAsString := ""
	switch command {
	case store.Deactivate:
		requestedStateAsString = "inactive"
	case store.Activate:
		requestedStateAsString = "active"
	}

	numUpdates := 0
	for _, id := range ticketIds {
		logger.WithFields(logrus.Fields{
			"new_state": requestedStateAsString,
		}).Debugf("update ticket status request id: %v", id)
		tc.UpRequests <- &cache.UpdateRequest{
			// This command (adding/removing the id to the inactive list)
			// is replicated to all other om-core instances using the batch
			// writing async goroutine tc.OutgoingReplicationQueue() and its
			// effect is applied to the local ticket cache in the update
			// processing async goroutine tc.IncomingReplicationQueue().
			ResultsChan: rChan,
			Ctx:         parentCtx,
			Update: store.StateUpdate{
				Cmd: command,
				Key: id,
			},
		}
		numUpdates++
	}

	// look through all results for errors
	for i := 0; i < len(ticketIds); i++ {
		results := <-rChan

		if results.Err != nil {
			// Wrap redis error and give it a gRPC internal server error status code
			// The results.result field contains the ticket id that generated the error.
			errs[results.Result] = fmt.Errorf("Unable to update ticket %v state to %v : %w", ticketIds[i], requestedStateAsString, results.Err)
			logger.Error(errs[results.Result])
		}
	}
	return errs
}

// ActivateTickets accepts a list of ticketids to activate, validates the
// input, and generates replication updates for each activation event.
func (s *grpcServer) ActivateTickets(parentCtx context.Context, req *pb.ActivateTicketsRequest) (*pb.ActivateTicketsResponse, error) {

	// Validate number of requested updates
	numReqUpdates := len(req.GetTicketIds())
	if numReqUpdates == 0 {
		err := status.Error(codes.InvalidArgument, "No Ticket IDs in update request")
		return nil, err
	}
	if numReqUpdates > cfg.GetInt("OM_MAX_STATE_UPDATES_PER_CALL") {
		errMsg := fmt.Sprintf("Too many ticket activations requested in a single call (configured maximum %v, requested %v)",
			cfg.GetInt("OM_MAX_STATE_UPDATES_PER_CALL"), numReqUpdates)
		err := status.Error(codes.InvalidArgument, errMsg)
		return nil, err
	}

	// Validate input against state storage event id format
	validTicketIds, invalidTicketIds := validateTicketIds(req.GetTicketIds())

	// Send the ticket activation updates. Returns the last Replication ID of
	// the batch. Can be used to determine if all of these activations have
	// been applied to a given om-core instance.
	errs := updateTicketsActiveState(parentCtx, validTicketIds, store.Activate)

	// Add activation failures to the error details
	_ = invalidTicketIds
	err := addStateUpdateErrorDetails(errs)

	// Record metrics
	ActivationsPerCall.Record(parentCtx, int64(len(validTicketIds)))

	return &pb.ActivateTicketsResponse{}, err
}

// DeactivatesTicket is a lazy deletion process: it adds the provided ticketIDs to the inactive list,
// which prevents them from appearing in player pools, and the tickets are deleted when they expire.
func (s *grpcServer) DeactivateTickets(parentCtx context.Context, req *pb.DeactivateTicketsRequest) (*pb.DeactivateTicketsResponse, error) {

	// Validate number of requested updates
	numReqUpdates := len(req.GetTicketIds())
	if numReqUpdates == 0 {
		err := status.Error(codes.InvalidArgument, "No Ticket IDs in update request")
		return nil, err
	}
	if numReqUpdates > cfg.GetInt("OM_MAX_STATE_UPDATES_PER_CALL") {
		errMsg := fmt.Sprintf("Too many ticket deactivations requested in a single call (configured maximum %v, requested %v)",
			cfg.GetInt("OM_MAX_STATE_UPDATES_PER_CALL"), numReqUpdates)
		err := status.Error(codes.InvalidArgument, errMsg)
		return nil, err
	}
	// Validate input against state storage event id format
	validTicketIds, invalidTicketIds := validateTicketIds(req.GetTicketIds())

	// Send the ticket deactivation updates. Returns the last Replication ID of
	// the batch. Can be used to determine if all of these activations have
	// been applied to a given om-core instance.
	errs := updateTicketsActiveState(parentCtx, validTicketIds, store.Deactivate)

	// Add deactivation failures to the error details
	_ = invalidTicketIds
	err := addStateUpdateErrorDetails(errs)

	// Record metrics
	DeactivationsPerCall.Record(parentCtx, int64(len(validTicketIds)))

	return &pb.DeactivateTicketsResponse{}, err
}

// InvokeMatchmakingFunctions loops through each Pool in the provided Profile,
// applying the filters inside and adding participating tickets to those pools.
// It then attempts to connect to every matchmaking function in the provided
// list, and send the Profile with filled Pools to each. It processes resulting
// matches from each matchmaking function asynchronously as they arrive. For
// each match it receives, it deactivates all tickets contained in the match
// and streams the match back to the InvokeMatchmakingFunctions caller.
//
// TODO: audit context/cancellation
//
//	https://stackoverflow.com/questions/76724124/does-a-go-grpc-server-streaming-method-not-have-a-context-argument
func (s *grpcServer) InvokeMatchmakingFunctions(req *pb.MmfRequest, stream pb.OpenMatchService_InvokeMatchmakingFunctionsServer) error {

	// input validation
	if req.GetProfile() == nil {
		return status.Error(codes.InvalidArgument, "profile is required")
	}
	if req.GetMmfs() == nil {
		return status.Error(codes.InvalidArgument, "list of mmfs to invoke is required")
	}

	logger := logger.WithFields(logrus.Fields{
		"stage":        "handling",
		"rpc":          "InvokeMatchmakingFunctions",
		"profile_name": req.GetProfile().GetName(),
	})

	// Apply filters from all pools specified in this profile
	// to find the participating tickets for each pool.  Start by snapshotting
	// the state of the ticket cache to a new data structure, so we can work
	// with that snapshot without incurring a bunch of additional access
	// contention on the ticket cache itself, which will continue to be updated
	// as we process. The participants of these pools won't reflect updates to
	// the ticket cache that happen after this point.

	// Copy the ticket cache, leaving out inactive tickets.
	activeTickets := setDifference(&tc.Tickets, &tc.InactiveSet)
	// TODO: this is just for metrics, move to a proper metric counter var instead
	unassignedTickets := setDifference(&tc.InactiveSet, &tc.Assignments)

	logger.Infof(" %5d tickets active, %5d tickets inactive without assignment",
		len(activeTickets), len(unassignedTickets))
	logger.Debugf("Ticket cache contains %v active tickets to filter into pools", len(activeTickets))

	// validate pool filters before filling them
	validPools := map[string][]*pb.Ticket{}
	for name, pool := range req.GetProfile().GetPools() {
		if valid, err := filter.ValidatePoolFilters(pool); valid {
			// Initialize a clean roster for this pool
			validPools[name] = make([]*pb.Ticket, 0)
		} else {
			logger.Error("Unable to fill pool with tickets, invalid: %w", err)
		}
	}
	logger.Debugf("Tickets filtered into %v pools", len(validPools))

	// Perform filtering, and 'chunk' the pools into 4mb pieces for streaming
	// (4mb is default max pb size.)
	// 1000 instead of 1024 for a little extra headroom, we're not trying to hyper-optimize here
	// Every chunk contains the entire profile, and a portion of the tickets in that profile's pools.
	maxPbSize := 4 * 1000 * 1000
	// Figure out how big the message is with the profile populated, but all pools empty
	emptyChunkSize := proto.Size(&pb.ChunkedMmfRunRequest{Profile: req.GetProfile(), NumChunks: math.MaxInt32})
	curChunkSize := emptyChunkSize

	// Array of 'chunks', each consisting of a portion of the pools in this profile.
	// MMFs need to re-assemble the pools with a simple loop+concat over all chunks
	var chunkCount int32
	chunkedPools := make([]map[string][]*pb.Ticket, 0)
	chunkedPools = append(chunkedPools, map[string][]*pb.Ticket{})

	for _, ticket := range activeTickets {
		for name, _ := range validPools {
			// All the implementation details of filtering are in internal/filter/filter.go
			if filter.In(req.GetProfile().GetPools()[name], ticket.(*pb.Ticket)) {
				ticketSize := proto.Size(ticket.(*pb.Ticket))
				// Check if this ticket will put us over the max pb size for this chunk
				if (curChunkSize + ticketSize) >= maxPbSize {
					// Start a new chunk
					curChunkSize = emptyChunkSize
					chunkCount++
					chunkedPools = append(chunkedPools, map[string][]*pb.Ticket{})
				}
				chunkedPools[chunkCount][name] = append(chunkedPools[chunkCount][name], ticket.(*pb.Ticket))
				curChunkSize += ticketSize
			}
		}
	}
	logger.Debugf("%v pools split into %v chunks", len(validPools), len(chunkedPools))

	// put final participant rosters into the pools.
	// Send the full profile in each streamed 'chunk', only the pools are broken
	// up to keep the pbs under the max size. This could probably be optimized
	// so we don't repeatedly send profile details in larger payloads, but this
	// implementation is 1) simpler and 2) could still be useful to the receiving
	// MMF if it somehow only got part of the chunked request.
	chunkedRequest := make([]*pb.ChunkedMmfRunRequest, len(chunkedPools))
	for chunkIndex, chunk := range chunkedPools {
		// Fill this request 'chunk' with the chunked pools we built above
		pools := make(map[string]*pb.Pool)
		profile := &pb.Profile{
			Name:       req.GetProfile().GetName(),
			Pools:      pools,
			Extensions: req.GetProfile().GetExtensions(),
		}
		for name, participantRoster := range chunk {
			profile.GetPools()[name] = &pb.Pool{
				Participants: &pb.Roster{
					Name:    name + "_roster",
					Tickets: participantRoster,
				},
			}
		}
		chunkedRequest[chunkIndex] = &pb.ChunkedMmfRunRequest{
			Profile:   profile,
			NumChunks: int32(len(chunkedPools)),
		}
	}

	// MMF Result fan-in goroutine
	// Simple fan-in channel pattern implemented as an async inline goroutine.
	// Asynchronously sends matches to InvokeMatchMakingFunction() caller as
	// they come in from the concurrently-running MMFs. Exits when all
	// MMFs are complete.

	// Channel on which MMFs return their matches.
	matchChan := make(chan *pb.Match)
	// Channel that receives a bool flag when all MMFs have closed their streams.
	waitChan := make(chan bool)
	var mmfwg sync.WaitGroup
	go func() {
		logger.Debug("MMF results fan-in goroutine active")
		for {
			select {
			case match := <-matchChan:
				logger.WithFields(logrus.Fields{
					"match_id": match.GetId(),
				}).Debug("streaming back match")
				stream.Send(&pb.StreamedMmfResponse{Match: match})
			case <-waitChan:
				logger.Debug("ALL MMFS COMPLETE")
				close(matchChan)
				return
			}
		}
	}()

	// Set up grpc dial options for all MMFs
	// Generally, we want to allow long-running MMFs to continue until they
	// finish processing; but best practice dictates /some/ timeout here
	// (default 10 mins).
	var opts []grpc.DialOption
	mmfTimeout := time.Duration(cfg.GetInt("OM_MMF_TIMEOUT_SECS")) * time.Second
	opts = append(opts,
		grpc.WithConnectParams(grpc.ConnectParams{MinConnectTimeout: mmfTimeout}),
		grpc.WithTransportCredentials(insecure.NewCredentials()),
	)

	// Invoke each requested MMF, and put the matches they stream back into the match channel.
	for _, mmf := range req.GetMmfs() {
		// Add this invocation to the MMF wait group.
		// TODO: Change to an errorgroup
		mmfwg.Add(1)

		// MMF fan-out goroutine
		// Call the MMF asynchronously, so all processing happens concurrently
		go func(mmf *pb.MatchmakingFunctionSpec) error {
			defer mmfwg.Done()
			// var init
			var err error
			err = nil

			// Connect to gRPC server for this mmf.
			mmfHost := mmf.GetHost()
			if mmf.GetPort() > 0 {
				mmfHost = fmt.Sprintf("%v:%v", mmfHost, mmf.GetPort())
			}
			if mmf.GetType() == pb.MatchmakingFunctionSpec_REST {
				// OM1 allowed MMFs to use HTTP RESTful grpc implementations,
				// but its usage was incredibly low. This is where that should
				// be re-implemented if we see enough user demand.
				err = status.Error(codes.Internal, fmt.Errorf("REST Mmf invocation NYI %v: %w", mmfHost, err).Error())

			} else { // TODO: gRPC is default for legacy OM1 reasons, need to swap enum order in pb to change

				logger := logger.WithFields(logrus.Fields{
					"mmf_name": mmf.GetName(),
					"mmf_addr": mmfHost,
				})

				// TODO: Future Optimization: cache connections/clients using a pool and re-use
				var conn *grpc.ClientConn
				conn, err = grpc.Dial(mmfHost, opts...)
				if err != nil {
					logger.Errorf("Error connecting to MMF %v", err)
					return status.Error(codes.Internal, fmt.Errorf("Failed to make gRPC client connection for %v: %w", mmfHost, err).Error())
				}
				defer conn.Close()
				client := pb.NewMatchMakingFunctionServiceClient(conn)
				logger.Debugf("connected to MMF")

				// Set a timeout for this API call, but use the empty
				// background context. It is fully intended that MMFs can run
				// longer than the matchmaker is willing to wait, so we want
				// them not to get cancelled when the calling matchmaker
				// cancels its context. However, best practices dictate that
				// we define /some/ timeout (default: 10 mins)
				ctx, cancel := context.WithTimeout(context.Background(), mmfTimeout)
				defer cancel()

				// Run the MMF
				var mmfStream pb.MatchMakingFunctionService_RunClient
				mmfStream, err = client.Run(ctx)
				if err != nil {
					return status.Error(codes.Internal, fmt.Errorf("Failed to connect to MMF at %v: %w", mmfHost, err).Error())
				}
				logger.Debugf("waiting for results")

				// Request itself is chunked if all the tickets returned in
				// ticket pools result in a total request size larger than the
				// default gRPC message size of 4mb.
				for index, chunk := range chunkedRequest {
					mmfStream.Send(chunk)
					logger.Debugf("MMF request chunk %02d/%02d: %0.2fmb", index+1, len(chunkedRequest), float64(proto.Size(chunk))/float64(1024*1024))
				}

				// Make a waitgroup that lets us know when all ticket
				// deactivations are complete. (All tickets in matches returned
				// by the MMF are set to inactive by OM)
				var tdwg sync.WaitGroup

				// i counts the number of loops for metrics reporting; this
				// loop reads streaming match responses from the mmf and runs
				// until a break statement is encountered.
				var i int64
				for i = 1; true; i++ {
					// Get results from MMF
					var result *pb.StreamedMmfResponse
					result, err = mmfStream.Recv()

					// io.EOF is the error grpc sends when the server closes a stream.
					if errors.Is(err, io.EOF) {
						logger.Debugf("MMF stream complete")
						break
					}
					if err != nil { // MMF has an error
						logger.Errorf("MMF Error")
						return status.Error(codes.Internal, fmt.Errorf("Unable to invoke MMF '%v' at %v: %w", mmf.GetName(), mmfHost, err).Error())
					}

					// deactivate tickets & send match back to the matchmaker
					// asynchronously as streamed match results continue to be
					// processed.
					//
					// Note, even with the waitgroup, this implementation is
					// only waiting until the deactivations are replicated to
					// the  local ticket cache. The distributed/eventually
					// consistent nature of om-core implementation means there
					// is no feasible way to wait for replication to every
					// instance
					//
					// TODO: This section /could/ be optimized by
					// de-duplicating tickets before sending them for
					// deactivation, but we'd only expect big gains doing that
					// if there are /lots/ of tickets appearing in multiple
					// matches. Since we expect /most/ matchmakers to try and
					// minimize collisions, it's likely a premature
					// optimization to do this before we hear concrete feedback
					// that it is a performance bottleneck.
					resultCopy := result.GetMatch()
					tdwg.Add(1)
					go func(res *pb.Match) {
						defer tdwg.Done()

						// Get ticket ids to deactivate from all rosters in this match.
						ticketIdsToDeactivate := []string{}
						for _, roster := range res.GetRosters() {
							for _, ticket := range roster.GetTickets() {
								ticketIdsToDeactivate = append(ticketIdsToDeactivate, ticket.GetId())
							}
						}

						// Kick off deactivation
						logger.Debugf("deactivating tickets in %v", res.GetId())
						errs := updateTicketsActiveState(ctx, ticketIdsToDeactivate, store.Deactivate)
						if len(errs) > 0 {
							logger.Errorf("Error deactivating match %v tickets: %v", res.GetId(), err)
						}
						logger.Debugf("Done deactivating tickets in %v", res.GetId())

						// Function to check if deactivation of the last ticket
						// in this match's rosters has been replicated to the
						// local cache
						deactivationCheck := func(ticketId string) {
							// We'd never expect the deactivation to take this
							// long to replicate, but best practices require
							// /some/ timeout.
							timeout := time.After(time.Second * 60)
							select {
							case <-timeout:
								// Log the timeout and continue.
								logger.Errorf("Timeout while waiting for ticket %v deactivation to be replicated to local cache", ticketId)
								return
							default:
								// There is always /some/ replication delay,
								// sleep first
								time.Sleep(100 * time.Millisecond)
								if _, replComplete := tc.InactiveSet.Load(ticketId); replComplete == true {
									logger.Debugf("deactivation of ticket %v replicated to local cache", ticketId)
									return
								}
							}
						}

						// Option 1: Wait for deactivation to complete BEFORE
						// returning the match.
						if cfg.GetBool("OM_MATCH_TICKET_DEACTIVATION_WAIT") {
							deactivationCheck(ticketIdsToDeactivate[len(ticketIdsToDeactivate)-1])
							logger.Debug("ticket deactivations complete, returning match")
						}

						// Send back the match on the channel, which is consumed
						// in the MMF Result fan-in goroutine above.
						logger.WithFields(logrus.Fields{
							"match_id": res.GetId(),
						}).Debugf("sending match to InvokeMMF() output queue")
						matchChan <- res

						// Option 2: Match already returned; clean up
						// goroutine when deactivation is complete.
						if !cfg.GetBool("OM_MATCH_TICKET_DEACTIVATION_WAIT") {
							deactivationCheck(ticketIdsToDeactivate[len(ticketIdsToDeactivate)-1])
							logger.Debug("ticket deactivations complete for previously returned match")
						}
						return
					}(resultCopy) // End of match return & ticket deactivation goroutine

				} // End of match stream receiving loop

				// TODO decide if we want to track profile names as attributes
				// as well as mmf names. Profile names are user input, so we'd
				// have to limit cardinality somehow. Really we should address
				// the possiblity of too many mmf names causing cardinality
				// problems too, but it's a relatively low-risk assumption that
				// the user will only deploy a few different mmfs.
				Matches.Add(ctx, i, metric.WithAttributes(attribute.String("mmf.name", mmf.GetName())))

				// Wait for all match ticket deactivation goroutines to complete.
				tdwg.Wait()
			}

			// io.EOF indicates the MMF server closed the stream (mmf is
			// complete), so don't log if that's the type of error in var err
			// (it's not a failure)
			if err != nil && !errors.Is(err, io.EOF) {
				logger.Error(err)
			}
			logger.Debug("async call to mmf complete")
			return err
		}(mmf) // End of MMF fan-out goroutine
	}

	// TODO switch to an errgroup that works with contexts.
	// https://stackoverflow.com/questions/71246253/handle-goroutine-termination-and-error-handling-via-error-group
	// Wait for all mmfs to complete.
	mmfwg.Wait()
	logger.Debugf("MMF waitgroup done; closing waitchan")
	close(waitChan)
	return nil
}

// CreateAssignments should be considered deprecated, and is only provided for
// use by developers when programming against om-core. In production, having
// the matchmaker handle this functionality doesn't make for clean failure
// domains and introduces unnecessary load on the matchmaker.
// Functionally, CreateAssignments makes replication updates for each ticket in
// the provided roster, assigning it to the server provided in the roster's
// Assignment field.
func (s *grpcServer) CreateAssignments(parentCtx context.Context, req *pb.CreateAssignmentsRequest) (*pb.CreateAssignmentsResponse, error) {
	logger := logger.WithFields(logrus.Fields{
		"stage": "handling",
		"rpc":   "CreateAssignments",
	})

	// Input validation
	if req.GetAssignmentRoster() == nil || req.GetAssignmentRoster().GetAssignment() == nil {
		return nil, status.Error(codes.InvalidArgument, "roster with assignment is required")
	}
	assignmentPb, err := proto.Marshal(req.GetAssignmentRoster().GetAssignment())
	if assignmentPb == nil || err != nil {
		err = errors.Wrap(err, "failed to marshal the assignment protobuf")
		logger.Errorf("Error: %v", err)
		return nil, err
	}

	rChan := make(chan *store.StateResponse, len(req.GetAssignmentRoster().GetTickets()))
	numUpdates := 0
	for _, ticket := range req.GetAssignmentRoster().GetTickets() {
		if ticket.Id != "" {
			logger.Debugf("Assignment request ticket id: %v", ticket.Id)
			tc.UpRequests <- &cache.UpdateRequest{
				// This command is replicated to all other om-core instances
				// using the batch writing async goroutine
				// tc.OutgoingReplicationQueue() and its effect is applied to the
				// local ticket cache in the update processing async goroutine
				// tc.IncomingReplicationQueue().
				Update: store.StateUpdate{
					Cmd:   store.Assign,
					Key:   ticket.Id,
					Value: string(assignmentPb[:]),
				},
				ResultsChan: rChan,
				Ctx:         parentCtx,
			}
			numUpdates++
		} else {
			// Note the error but continue processing
			err = status.Error(codes.Internal, "Empty ticket ID in assignment request!")
			logger.Error(err)
		}
	}

	for i := 0; i < numUpdates; i++ {
		// The results.result field contains the redis stream id (aka the replication ID)
		// We don't actually need this for anything, so just check for an error
		results := <-rChan

		if results.Err != nil {
			// Wrap redis error and give it a gRPC internal server error status code
			err = status.Error(codes.Internal, fmt.Errorf("Unable to delete ticket: %w", results.Err).Error())
			logger.Error(err)
		}
	}

	logger.Debugf("DEPRECATED CreateAssignments: %v tickets given assignment \"%v\"", numUpdates, req.GetAssignmentRoster().GetAssignment().GetConnection())

	// Record metrics
	AssignmentsPerCall.Record(parentCtx, int64(numUpdates))

	return &pb.CreateAssignmentsResponse{}, nil
}

// WatchAssignments should be considered deprecated, and is only provided for
// use by developers when programming against om-core. In production, having
// the matchmaker handle this functionality doesn't make for clean failure
// domains and introduces unnecessary load on the matchmaker.
// Functionally, it asynchronously watches for exactly one replication update
// containing an assignment for each of the provided ticket ids, and streams
// those assignments back to the caller.  This is rather limited functionality
// as reflected by this function's deprecated status.
func (s *grpcServer) WatchAssignments(req *pb.WatchAssignmentsRequest, stream pb.OpenMatchService_WatchAssignmentsServer) error {
	logger := logger.WithFields(logrus.Fields{
		"stage": "handling",
		"rpc":   "WatchAssignments",
	})

	logger.Debugf("ticketids to watch: %v", req.GetTicketIds())

	// var init
	newestTicketCtime := time.Now()
	updateChan := make(chan *pb.StreamedWatchAssignmentsResponse)
	defer close(updateChan)

	for _, id := range req.GetTicketIds() {
		// Launch an asynch goroutine for each ticket ID in the request that just loops,
		// checking the sync.Map for an assignment. Once one is found, it puts that in the
		// channel, then exits.
		go func(ticketId string) {
			for {
				if value, ok := tc.Assignments.Load(ticketId); ok {
					assignment := value.(*pb.Assignment)
					logger.Debugf(" watchassignments loop got assignment %v for ticketid: %v",
						assignment.Connection, ticketId)
					updateChan <- &pb.StreamedWatchAssignmentsResponse{
						Assignment: assignment,
						Id:         ticketId,
					}
					return
				}
				// TODO exp bo + jitter. Not a priority since this API is
				// marked as deprecated.
				time.Sleep(1 * time.Second)
			}
		}(id)

		// Get creation timestamp from ID
		x, err := strconv.ParseInt(strings.Split(id, "-")[0], 0, 64)
		if err != nil {
			logger.Error(err)
		}
		ticketCtime := time.Unix(x, 0)

		// find the newest ticket creation time.
		if ticketCtime.After(newestTicketCtime) {
			newestTicketCtime = ticketCtime
		}
	}
	AssignmentWatches.Add(context.Background(), int64(len(req.GetTicketIds())))

	// loop once per result we want to get.
	// Exit when we've gotten all results or timeout is reached.
	// From newest ticket id, get the ticket creation time and deduce the max
	// time to wait from it's creation time + configured TTL
	// This is a very naive implementation. This functionality is deprecated.
	timeout := time.After(time.Until(newestTicketCtime.Add(time.Second * time.Duration(cfg.GetInt("OM_CACHE_TICKET_TTL_SECS")))))
	for i, _ := range req.GetTicketIds() {
		select {
		case thisAssignment := <-updateChan:
			AssignmentWatches.Add(context.Background(), -1)
			stream.Send(thisAssignment)
		case <-timeout:
			AssignmentWatches.Add(context.Background(), int64(len(req.GetTicketIds())-i))
			return nil
		}
	}

	return nil
}

// validateTicketIds checks each id in the input list, and sends back
// separate lists of those that are valid and those that are invalid.
func validateTicketIds(ids []string) (validIds, invalidIds []string) {
	logger := logger.WithFields(logrus.Fields{
		"stage": "validation",
	})

	validationRegex := regexp.MustCompile(tc.Replicator.GetReplIdValidationRegex())
	for _, id := range ids {
		logger.Debugf("Validating ticket id %v", id)
		if validationRegex.MatchString(id) {
			validIds = append(validIds, id)
			logger.Debugf("ticket id %v VALID", id)
		} else {
			invalidIds = append(invalidIds, id)
			logger.Error(status.Error(codes.InvalidArgument, "Ticket ID not valid. Please pass a valid Ticket ID.").Error())
		}
	}
	return
}

// addStateUpdateErrorDetails is a convenience funtion to pass back detailed
// errors for API calls that allow the client to pass in multiple updates at once.
// gRPC API guidelines explains more about best practices using these.
// https://cloud.google.com/apis/design
//
// TODO: This is a POC implemention and needs to be expanded and finalized once
// we understand the kinds of potential errors it is passing back.
func addStateUpdateErrorDetails(errs map[string]error) error {
	st := status.New(codes.OK, "")
	if len(errs) > 0 {
		st = status.New(codes.InvalidArgument, "One or more state update arguments were invalid")

		// Generate error details
		deets := &errdetails.BadRequest{}
		for ticketId, e := range errs {
			deets.FieldViolations = append(deets.FieldViolations, &errdetails.BadRequest_FieldViolation{
				Field:       fmt.Sprintf("ticketId/%v", ticketId),
				Description: e.Error(),
			})
		}

		// Add error details
		var err error
		st, err = st.WithDetails(deets)
		if err != nil {
			logger.Errorf("Unexpected error while updateing tickets: %v", err)
		}
	}
	return st.Err()
}

// syncMapDump is a simple helper function to convert a sync.Map into a
// standard golang map. Since a standard map is not concurrent-safe, use this
// with caution. Most of the places this is used are either for debugging the
// internal state of the om-core ticket cache, or in portions of the code where
// the implementation depends on taking a 'point-in-time' snapshot of the
// ticket cache because we're sending it to another process using invokeMMF()
func syncMapDump(sm *sync.Map) map[string]interface{} {
	out := map[string]interface{}{}
	sm.Range(func(key, value interface{}) bool {
		out[fmt.Sprint(key)] = value
		return true
	})
	return out
}

// setDifference is a simple helper function that performs a difference
// operation on two sets that are using sync.Map as their underlying data type.
//
// NOTE the limitation that this is taking a 'point-in-time' snapshot of the
// two sets, as they are still being updated asynchronously in a number of
// other goroutines. The rest of the design of OM takes this into account.
func setDifference(tix *sync.Map, inactiveSet *sync.Map) (activeTickets []any) {
	inactiveTicketIds := syncMapDump(inactiveSet)
	// Copy the ticket cache, leaving out inactive tickets.
	tix.Range(func(id, ticket any) bool {
		// not ok means an error was encountered, indicating this
		// ticket is NOT inactive (meaning it IS active)
		if _, ok := inactiveTicketIds[id.(string)]; !ok {
			activeTickets = append(activeTickets, ticket)
		}
		return true
	})
	return
}
